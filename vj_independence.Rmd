---
title: "Independence of V and J Primers"
author: "Wes Horton, Burcu Gurun-Demir"
date: "May 4, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reshape2)
library(MASS)
```

## Summary of Dataset and Purpose

We have approximately 170 samples per sequencing batch, and for each sample we have 260 counts, one for each of the unique combinations of V and J primers. The primers have different amplification rates, which we need to characterize. In order to do this most accurately, we need to determine if the forward (V) primer and the reverse (J) primer act independently to influence spike amplification, or if their interaction is important as well.  

Two different PCR batches comprise our sequencing batch. There may be a difference in amplification biases based on these batches as well. In addition, samples were diluted by varying degrees prior to sequencing, and we need to determine if that had an influence.  

### Variables

1. Independent variables
    + Forward (V) primer identity - 20 total
    + Reverse (J) primer identity - 13 total
    + PCR batch identity - 2 total
    + Tape Station dilution factor  
  
  
2. Dependent variable
    + Spike Count  


Each sample has an individual file containing the 260 counts. These need to be combined into a single data frame prior to the analysis.

```{r cars}
# Set directory
count.dir <- "~/Desktop/OHSU/tcr_spike/data/vj_counts/DNA150826/"

# Read in files and sort by sample number
all.counts <- list.files(count.dir)
all.counts <- all.counts[order(as.numeric(gsub(".*_S|.assembled.*", '', all.counts)))]

# Read in first file to start aggregate data frame
count.df <- read.table(file.path(count.dir, all.counts[1]), sep =',', header = T)
count.df <- count.df[,3:5]

# Comine spike counts for all files into 1 data frame
  # Columns are samples
  # Rows are spikes
for (i in 2:length(all.counts)){
  curr.df <- read.table(file.path(count.dir, all.counts[i]), sep = ',', header = T)
  count.df <- cbind(count.df, curr.df$spike.count)
}   #   for i in 2:length(new.counts)
colnames(count.df) <- c("V", "J", seq(1:length(all.counts)))

head(count.df[,1:10], n = 20)

# Collapse data frame to 1 count column
melt.count.df <- melt(count.df, id.vars = c("V", "J"))

# Add pseudo-variable of V/J combos
melt.count.df$combos <- paste(melt.count.df$V, melt.count.df$J, sep = '')

# Take log2 of count values due to geometric distribution
log2.melt.count.df <- melt.count.df
log2.melt.count.df$value <- log2(melt.count.df$value + 1)


head(log2.melt.count.df, n = 20)
# V is all V segments, repeated for each J for each sample
# J is all J segments, repeated same as V
# Variable corresponds to sample number
# Value is log2 of count
```




## Linear Regression Model

We want to create a linear regression model in order to test the interaction effect between V and J primers on spike counts. First we'll show an additive model, then a multiplicative model.

```{r}
# Additive Model
add.lm <- lm(value ~ V + J, log2.melt.count.df)
summary(add.lm)
add.adj.r2 <- round(summary(add.lm)$adj.r.squared, digits = 4)

# Multiplicative Model
mult.lm <- lm(value ~ V * J, log2.melt.count.df)
summary(mult.lm)
mult.adj.r2 <- round(summary(mult.lm)$adj.r.squared, digits = 4)
paste("Additive R^2:", add.adj.r2, sep = ' ')
paste("Multiplicative R^2:", mult.adj.r2, sep = ' ')
```

We can see that the multiplicative model explains more variation than the additive. Now we should look at specific V/J pairs to see if there are any specific combinations that are contributing to this increase.

```{r}
# Specify Null Model
null <- lm(value ~ 1, log2.melt.count.df)
summary(null)

# Specify Full Model
full <- lm(value ~ V + J + combos, data = log2.melt.count.df)
summary(full)
attr(summary(full)$aliased, "names")

# This freezes R - too many combinations I think.
#full3 <- lm(value ~ V * J * combos, data = log2.melt.count.df)
#summary(full3)

# Now Attempt step functions
# Forward
stepforward <- step(null, scope = list(lower = null, upper = full), direction = "forward")
# Not sure why this only goes through:
# value ~ 1
# and
# value ~ combos

summary(stepforward)
# says call is value ~ comos

# Backward
stepbackward <- step(full, scope = list(lower =  null, upper = full), direction = "backward")
# AIC is the same for all of these...
summary(stepbackward)
# Says call is same as forward

# Both
stepboth <- step(full, scope = list(lower = null, upper = full), direction = "both")
# Only goes one direction
summary(stepboth)
# Same call as forward and backward

```


Also going to do a chi squared test. Need to format the data so that we have V's as rows and J's as columns

```{r}
#row.col <- paste(count.df$V, count.df$J, sep = '')
#chi.sq.df2 <- data.frame(matrix(nrow = length(row.col), ncol = length(row.col)))
#rownames(chi.sq.df2) <- row.col
#colnames(chi.sq.df2) <- row.col
# Need to create a 20 x 13 matrix of V and J counts for chi-squared test
vs <- unique(count.df$V)
js <- unique(count.df$J)
v.j.df <- data.frame(matrix(nrow = length(vs), ncol = length(js)))
rownames(v.j.df) <- vs
colnames(v.j.df) <- js

# Run Chi-squared on each individually and extract p value
chi.sq.ps <- NULL
for (i in 3:length(names(count.df))){
  curr.df <- count.df[,c(1:2,i)]
  curr.vj <- populate.vj.df(curr.df, v.j.df)
  curr.chi <- chisq.test(curr.vj, correct = F)
  curr.p <- curr.chi$p.value
  chi.sq.ps <- c(chi.sq.ps, curr.p)  
}

# Make another to hold scaled values
scaled.v.j.df <- v.j.df

# Extract counts only
#subset.count <- count.df[,3:169]

# Transform subset data frame so that rows are samples - do we need to do this?
#subset.count <- t(subset.count)

# Extract row totals
# This is a sum of counts for all spikes of a given sample
#sum.of.rows <- apply(subset.count, 1, sum)

# Extract column totals instead for untransformed df
#sum.of.samples <- apply(count.df[,3:169], 2, sum)

# Divide each cell in a row by its sum
#scaled.counts <- apply(subset.count, 2, function(x) x / sum.of.rows)
# Divide each cell in column by its sum instead
#scaled.counts <- apply(count.df[,3:169], 1, function(x) x / sum.of.samples)
#rownames(scaled.counts) <- seq(1:167)
#colnames(scaled.counts) <- paste(count.df$V, count.df$J, sep = '')

# Sum each row
count.sums.by.sample <- apply(count.df[,3:169], 1, sum)

# Recombine with names
count.sums.by.sample <- cbind(count.df[,1:2], count.sums.by.sample)

# Transform and paste to V/J names
#new.scaled.counts <- cbind(count.df[,1:2], count.sums.by.sample)

# Create data frame with 260 rows (1 for each VJ combo, and a total count)
# This will be transformed to a 20 x 13 dataframe with cells as counts
#new.count <- cbind(count.df[,1:2], sum.of.rows)

# Do the same for the scaled data frame
#new.scaled.counts <- cbind(count.df[,1:2], sample.sums)

# Function to populate df by matching V and J strings
populate.vj.df <- function(df.of.counts, vj.df){
  for (i in 1:length(df.of.counts[,3])){
    v <- as.character(df.of.counts[i,1])
    j <- as.character(df.of.counts[i,2])
    sum <- df.of.counts[i,3]
    rownum <- which(v == row.names(vj.df))
    colnum <- which(j == colnames(vj.df))
    vj.df[rownum,colnum] <- sum
  }  # for
  return(vj.df)
}  # populate.vj.df(df.of.counts, vj.df)

# Call function
v.j.df <- populate.vj.df(count.sums.by.sample, v.j.df)
scaled.v.j.df <- populate.vj.df(new.scaled.counts, scaled.v.j.df)

new.scaled.v.j.df <- apply(scaled.v.j.df, c(1,2), function(x) x * 10000)

# Run Chi square
orig.chi <- chisq.test(v.j.df, correct = F)
orig.chi
orig.chi$observed
orig.chi$expected

scaled.chi <- chisq.test(scaled.v.j.df, correct = F)
scaled.chi
scaled.chi$observed
scaled.chi$expected

scaled.chi.2 <- chisq.test(new.scaled.v.j.df, correct = T)
scaled.chi
scaled.chi$observed
scaled.chi$expected


# Calculate standardized residuals
scaled.std.resid <- (scaled.chi$observed - scaled.chi$expected) / (sqrt(scaled.chi$expected))

# Calulate adjusted standardized residuals
orig.adj.std.resid <- (orig.chi$observed - orig.chi$expected) /
                        sqrt(orig.chi$expected * ((1 - rowSums(orig.chi$observed) /
                                                        sum(orig.chi$observed))
                                                     %*% t(1 - colSums(orig.chi$observed) /
                                                             sum(orig.chi$observed))))

# Assess individual contributions


#######
#######
####### Old Stuff
# Sum rows - this is a sum of counts across all samples, 1 for each spike
sum.of.rows <- apply(subset.count, 1, sum)

# Take mean of each spike count across the samples
count.mean <- sum.of.rows / length(names(subset.count))

# Create scaling factor by dividing count mean by 1
scaling.factor <- 1 / count.mean

# Multiply each column (sample) by scaling factor
# This should cause each row to be multiplied by the same scaling factor
scaled.counts <- apply(subset.count, 2, function(x) (x * scaling.factor))

# Sum scaled counts
sum.scaled.rows <- apply(scaled.counts, 1, sum)
