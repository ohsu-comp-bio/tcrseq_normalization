---
title: "Independence of V and J Primers"
author: "Wes Horton, Burcu Gurun-Demir"
date: "May 4, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reshape2)
library(MASS)
```

```{r, echo = F}
# Function to populate df by matching V and J strings
populate.vj.df <- function(df.of.counts, vj.df){
  for (i in 1:length(df.of.counts[,3])){
    v <- as.character(df.of.counts[i,1])
    j <- as.character(df.of.counts[i,2])
    sum <- df.of.counts[i,3]
    rownum <- which(v == row.names(vj.df))
    colnum <- which(j == colnames(vj.df))
    vj.df[rownum,colnum] <- sum
  }  # for
  return(vj.df)
}  # populate.vj.df(df.of.counts, vj.df)
```

## Summary of Dataset and Purpose

We have approximately 170 samples per sequencing batch, and for each sample we have 260 counts, one for each of the unique combinations of V and J primers. The primers have different amplification rates, which we need to characterize. In order to do this most accurately, we need to determine if the forward (V) primer and the reverse (J) primer act independently to influence spike amplification, or if their interaction is important as well.  

Two different PCR batches comprise our sequencing batch. There may be a difference in amplification biases based on these batches as well. In addition, samples were diluted by varying degrees prior to sequencing, and we need to determine if that had an influence.  

### Variables

1. Independent variables
    + Forward (V) primer identity - 20 total
    + Reverse (J) primer identity - 13 total
    + PCR batch identity - 2 total
    + Tape Station dilution factor  
  
  
2. Dependent variable
    + Spike Count  


Each sample has an individual file containing the 260 counts. These need to be combined into a single data frame prior to the analysis.

```{r cars}
# Set directory
count.dir <- "~/Desktop/OHSU/tcr_spike/data/vj_counts/DNA150826/"

# Read in files and sort by sample number
all.counts <- list.files(count.dir)
all.counts <- all.counts[order(as.numeric(gsub(".*_S|.assembled.*", '', all.counts)))]

# Read in first file to start aggregate data frame
count.df <- read.table(file.path(count.dir, all.counts[1]), sep =',', header = T)
count.df <- count.df[,3:5]

# Comine spike counts for all files into 1 data frame
  # Columns are samples
  # Rows are spikes
for (i in 2:length(all.counts)){
  curr.df <- read.table(file.path(count.dir, all.counts[i]), sep = ',', header = T)
  count.df <- cbind(count.df, curr.df$spike.count)
}   #   for i in 2:length(new.counts)
colnames(count.df) <- c("V", "J", seq(1:length(all.counts)))

head(count.df[,1:10], n = 20)

# Collapse data frame to 1 count column
melt.count.df <- melt(count.df, id.vars = c("V", "J"))

# Add pseudo-variable of V/J combos
melt.count.df$combos <- paste(melt.count.df$V, melt.count.df$J, sep = '')

# Take log2 of count values due to geometric distribution
log2.melt.count.df <- melt.count.df
log2.melt.count.df$value <- log2(melt.count.df$value + 1)


head(log2.melt.count.df, n = 20)
# V is all V segments, repeated for each J for each sample
# J is all J segments, repeated same as V
# Variable corresponds to sample number
# Value is log2 of count
```




## Linear Regression Model

We want to create a linear regression model in order to test the interaction effect between V and J primers on spike counts. First we'll show an additive model, then a multiplicative model.

```{r}
# Additive Model
add.lm <- lm(value ~ V + J, log2.melt.count.df)
summary(add.lm)
add.adj.r2 <- round(summary(add.lm)$adj.r.squared, digits = 4)

# Multiplicative Model
mult.lm <- lm(value ~ V * J, log2.melt.count.df)
summary(mult.lm)
mult.adj.r2 <- round(summary(mult.lm)$adj.r.squared, digits = 4)
paste("Additive R^2:", add.adj.r2, sep = ' ')
paste("Multiplicative R^2:", mult.adj.r2, sep = ' ')
```

We can see that the multiplicative model explains more variation than the additive. Now we should look at specific V/J pairs to see if there are any specific combinations that are contributing to this increase.

```{r}
# Specify Null Model
null <- lm(value ~ 1, log2.melt.count.df)
summary(null)

# Specify Full Model
full <- lm(value ~ V + J + combos, data = log2.melt.count.df)
summary(full)
attr(summary(full)$aliased, "names")

# This freezes R - too many combinations I think.
#full3 <- lm(value ~ V * J * combos, data = log2.melt.count.df)
#summary(full3)

# Now Attempt step functions
# Forward
stepforward <- step(null, scope = list(lower = null, upper = full), direction = "forward")
# Not sure why this only goes through:
# value ~ 1
# and
# value ~ combos

summary(stepforward)
# says call is value ~ comos

# Backward
stepbackward <- step(full, scope = list(lower =  null, upper = full), direction = "backward")
# AIC is the same for all of these...
summary(stepbackward)
# Says call is same as forward

# Both
stepboth <- step(full, scope = list(lower = null, upper = full), direction = "both")
# Only goes one direction
summary(stepboth)
# Same call as forward and backward

```

Here is a different step function attempt:
```{r}
# Need to rearrange the data frame
# First transform, then create new column with rowsums
primer.names <- paste(count.df$V, count.df$J, sep = '')

new.count <- cbind(primer.names,
                   count.df[,3:169], stringsAsFactors = F)
new.count <- as.data.frame(t(new.count), stringsAsFactors = F)
new.count <- new.count[c(2:168),]
new.count <- apply(new.count, c(1,2), function(x) as.numeric(x))
count.total <- apply(new.count, 1, sum)
colnames(new.count) <- primer.names
new.count <- cbind("total" = count.total, new.count)
new.count <- data.frame(new.count)

test.lm <- lm(total ~ ., new.count)

# This doesn't work either...
```


Also going to do a chi squared test. Need to format the data so that we have V's as rows and J's as columns

```{r}
# Need to create a 20 x 13 matrix of V and J counts for chi-squared test
# This empty matrix will be populated by counts
vs <- unique(count.df$V)
js <- unique(count.df$J)
v.j.df <- data.frame(matrix(nrow = length(vs), ncol = length(js)))
rownames(v.j.df) <- vs
colnames(v.j.df) <- js

# We have two options for running the chi-squared. We can sum the counts of all of the samples and do one chi-squared, or we can do 170 chi-squareds, one for each sample.

# Variance and standard deviation of each of the 260 spikes
spike.var <- apply(subset.count, 1, var)
spike.sd <- apply(subset.count, 1, sd)

# This is for the second option - 170 individual chi-squared tests:
# Run Chi-squared on each individually and extract p value
chi.sq.ps <- NULL
log.chi.sq.ps <- NULL
for (i in 3:length(names(count.df))){
  curr.df <- count.df[,c(1:2,i)]
  curr.vj <- populate.vj.df(curr.df, v.j.df)
  log.curr.vj <- log2(curr.vj + 1)
  curr.chi <- chisq.test(curr.vj, correct = F)
  curr.log.chi <- chisq.test(log.curr.vj, correct = F)
  curr.log.p <- curr.log.chi$p.value
  curr.p <- curr.chi$p.value
  chi.sq.ps <- c(chi.sq.ps, curr.p) 
  log.chi.sq.ps <- c(log.chi.sq.ps, curr.log.p)
}  # for
# Returns a vector of p-values for the chi-squared tests. They're all zero, which means that the null hypothesis of independence of primers is rejected in favor of dependent primers.


# Second option: Run chi-squared for the sum of all of the counts.
# We want to scale the counts within samples due to the high variation between samples.
# Extract counts only
# Rows are spikes, columns are samples
subset.count <- count.df[,3:169]


# Extract column totals - this is a sum of all 260 spike counts for a given sample
sum.of.samples <- apply(subset.count, 2, sum)

# Divide each cell in a sample by its sum
# Since the sum is a vector of length(samples) and our data frame is structured as columns = length(samples), we need to apply over each row, dividing by the entire vector.
scaled.counts <- apply(subset.count, 1, function(x) x / sum.of.samples)
# Rows are now samples and columns are spikes...
rownames(scaled.counts) <- seq(1:167)
colnames(scaled.counts) <- paste(count.df$V, count.df$J, sep = '')

# Variance and Standard Deviation of scaled counts
scaled.spike.var <- apply(scaled.counts, 2, var)
scaled.spike.sd <- apply(scaled.counts, 2, sd)

# Now that we've scaled each of the counts, we want to sum each row so that we get a total count for each of the 260 spikes, from all 170 samples. Remember, we need 260 values to populate our 20x13 chi-squared matrix.
# Sum each row
count.sums.by.sample <- apply(count.df[,3:169], 1, sum)

# Recombine with names for population
count.sums.by.sample <- cbind(count.df[,1:2], count.sums.by.sample)


# Call function
v.j.df <- populate.vj.df(count.sums.by.sample, v.j.df)



# Now use the 20 x 13 matrix to run a chi-squared
scaled.chi <- chisq.test(v.j.df, correct = F)
scaled.chi
# We get a p-value of 2.2 e -16, which tells us we should reject the null hypothesis that primers are independent, in favor of the alternative that they are dependent.
scaled.chi$observed
scaled.chi$expected

# We can take a look at the residuals to try and determine which ones are causing the dependence
# Calculate standardized residuals
scaled.std.resid <- (scaled.chi$observed - scaled.chi$expected) / (sqrt(scaled.chi$expected))

# Calulate adjusted standardized residuals
adj.std.resid <- round(((scaled.chi$observed - scaled.chi$expected) /
                        sqrt(scaled.chi$expected * ((1 - rowSums(scaled.chi$observed) /
                                                        sum(scaled.chi$observed))
                                                     %*% t(1 - colSums(scaled.chi$observed) /
                                                             sum(scaled.chi$observed))))), 
                       digits = 1)
write.csv(adj.std.resid, file = "~/Desktop/chi.sq.resids.csv", quote = F)

# Chi squared with log2 data
# Take the log2 of the counts
log2.v.j.df <- log2(v.j.df)

# Chi squared
log2.chi <- chisq.test(log2.v.j.df, correct = F)
log2.chi

# Calculate standardized residuals
log2.std.resid <- (log2.chi$observed - log2.chi$expected) / (sqrt(log2.chi$expected))

# Calulate adjusted standardized residuals
log2.adj.std.resid <- round(((log2.chi$observed - log2.chi$expected) /
                        sqrt(log2.chi$expected * ((1 - rowSums(log2.chi$observed) /
                                                        sum(log2.chi$observed))
                                                     %*% t(1 - colSums(log2.chi$observed) /
                                                             sum(log2.chi$observed))))), 
                       digits = 1)

#######
#######
####### Old Stuff
# Sum rows - this is a sum of counts across all samples, 1 for each spike
sum.of.rows <- apply(subset.count, 1, sum)

# Take mean of each spike count across the samples
count.mean <- sum.of.rows / length(names(subset.count))

# Create scaling factor by dividing count mean by 1
scaling.factor <- 1 / count.mean

# Multiply each column (sample) by scaling factor
# This should cause each row to be multiplied by the same scaling factor
scaled.counts <- apply(subset.count, 2, function(x) (x * scaling.factor))

# Sum scaled counts
sum.scaled.rows <- apply(scaled.counts, 1, sum)
